<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="IDSNLP 2024">
    <meta name="author" content="">
    <meta name="keywords" content="IDSNLP 2024, NUS, IDS, NLP, TheWebConf">
    <title>IDSNLP 2024: NUS IDS NLP-SIG Workshop for TheWebConf 2024</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <link rel="shortcut icon" type="image/x-icon" href="favicon.png">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" type="text/css" href="fonts/font-awesome-4.7.0/css/font-awesome.min.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="vendor/animate/animate.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="vendor/select2/select2.min.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="vendor/perfect-scrollbar/perfect-scrollbar.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="css/util.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">

    <style type="text/css">
        .navbar-text>a {
            color: inherit;
            text-decoration: none;
        }

        .white_bg {
            background-color: #eef7fa;
            padding: 3px;
        }

        .line2 {
            margin: 5px 0;
            height: 2px;
            background: repeating-linear-gradient(to right, black 0, black 10px, transparent 10px, transparent 12px)
                /*10px red then 2px transparent -> repeat this!*/
        }

        .bordered,
        .hover2,
        xximg:hover {
            border-color: #AAAAAA;
            border-style: solid;
            border-width: 1px;
            border-collapse: separate
                /* otherwise does not work in IE inside tables */
            ;
        }

        .hover2 {
            -webkit-box-shadow: 2px 2px 2px rgba(, , 120, 0.6);
            -moz-box-shadow: 2px 2px 2px rgba(, , 120, 0.6);
            -o-box-shadow: 2px 2px 2px rgba(, , 120, 0.6);
            box-shadow: 0px 0px 10px rgba(, , 120, 0.6);
        }

        figure figcaption {
            text-align: center;
            margin: 10px;
        }

        figure {
            display: inline-block;
            margin: 0px;
        }

        figure img {
            vertical-align: top;
            border: 1px solid #ddd;
            border-radius: 0px;
            padding: 0px;
        }

        figure img:hover {
            opacity: 0.7;
            filter: alpha(opacity=70);
            -webkit-box-shadow: -2px 4px 10px 0px rgba(0, 0, 0, 1);
            -moz-box-shadow: -2px 4px 10px 0px rgba(0, 0, 0, 1);
            box-shadow: -2px 4px 10px 0px rgba(0, 0, 0, 1);
            -webkit-transition: all .2s ease-in-out;
            transition: all .2s ease-in-out;

        }
    </style>
</head>

<body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand js-scroll-trigger" href="#">IDSNLP 2024</a>
            </div>

            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item"> <a class="nav-link js-scroll-trigger" href="#overview">Overview</a> </li>
                    <li class="nav-item"> <a class="nav-link js-scroll-trigger" href="#programme">Programme</a> </li>
                    <li class="nav-item"> <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a> </li>
                    <li class="nav-item"> <a class="nav-link js-scroll-trigger" href="#location">Location</a> </li>
                    <li class="nav-item"> <a class="nav-link js-scroll-trigger" href="#contact">Contact</a> </li>
                </ul>
            </div>
        </div>
    </nav>

    <header class="bg-primary text-white">
        <div class="container text-center">
            <h1 style="font-size: 60px;font-weight: bold;color: #e48a52;">IDSNLP 2024</h1>
            <h2 style="font-size: 35px;color: #ffffff;background-color: rgba(0, 123, 255, .25);">NUS IDS NLP-SIG
                Workshop for TheWebConf 2024</h2>
        </div>
    </header>

    <section id="overview" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h2>Welcome!</h2>
                    <p style="text-align: justify;">
                        We are pleased to announce that <b>the Singapore Symposium on Natural Language Processing (SSNLP
                            2023)</b> will be held on <b>Monday, December 4</b> (full day). This is an annual
                        Singapore-based pre-conference practice workshop for both local students, practitioners and
                        faculty working in Natural Language Processing to network. It has been successfully held in
                        2018, 2019, 2020, and 2022, becoming an increasingly popular and impactful event.
                    </p>

                    <p style="text-align: justify;">
                        We are further excited about the unique opportunity presented to Singapore on 6-10 December as
                        the <a href="https://2023.emnlp.org/">EMNLP 2023</a>, a premier Computational Linguistics and
                        NLP conference, is being held in Singapore. Leveraging the attendance of many reputed
                        academicians, we’re looking forward to hosting them as a part of SSNLP – and, further, taking
                        the opportunity to invite them to the School of Computing of National University of Singapore
                        (NUS) to engage with us.
                    </p>

                    <center>
                        <p> <a href="https://forms.gle/4wSc58KFvDoDz4Mn6" target="_blank"
                                class="btn  btn-primary">Register Now</a> </p>
                    </center>

                    <p style="text-align: justify;">
                        The venue will be held at <b>the Shaw Foundation Alumni House Auditorium, NUS (11 Kent Ridge Dr,
                            Singapore 119244, [<a
                                href="https://www.google.com/maps/place/Shaw+Foundation+Alumni+House/@1.2932317,103.7708068,17z/data=!3m2!4b1!5s0x31da1aff3ee4a48f:0x732a458e6add7ca5!4m6!3m5!1s0x31da1aff3f1cf5b1:0x7ae21f4141402cfd!8m2!3d1.2932263!4d103.7733817!16s%2Fg%2F11bx1h3zc_?entry=ttu">Map</a>],
                            [<a href="images/Shaw-1.jpg">Outdoor photo</a>], [<a href="images/Shaw-2.jpg">Indoor
                                photo</a>])</b>.
                        We are planning a hybrid (onsite & online) format for SSNLP to ensure maximal outreach.
                        Please note our venues can only accommodate a limited seating capacity; in the event of
                        oversubscription, we may offer online attendance to registered participants.
                        So make <b>early bird registration</b> as quick as possible to secure seats.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="programme" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h2>Programme</h2>

                    <p style="text-align: justify;">We've planned to host Oral and Post sessions for paper
                        presentations, and also have invited Keynote Presentations</p>

                    <div class="container-table100">
                        <div class="wrap-table100">

                            <div class="table100 ver5 m-b-10">
                                <table data-vertable="ver5">
                                    <thead>
                                        <tr class="row100 head">
                                            <th class="column100 column1" data-column="column1"><strong>Time</strong>
                                            </th>
                                            <th class="column100 column2" data-column="column2"><strong>Event</strong>
                                            </th>
                                        </tr>
                                    </thead>

                                    <tbody>
                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">8:30 - 8:45</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #228B22;">
                                                    <strong>Welcome and Opening Remarks</strong>
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; See-Kiong Ng (NUSß) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">8:45 - 9:15</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;"> 
                                                    <strong>When Spatio-Temporal Data Meet Large Language Models</strong> 
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; Yuxuan Liang (HKUST) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">9:15 - 9:45</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;">
                                                    <strong>Exploring Interaction Patterns of Sequence Data in Recommender Systems</strong>
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; Jiarui Jin (Xiaohongshu) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">9:45 - 10:30</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;">
                                                    <strong>LLM-based Clarification in Conversational Search: Going beyond Unimodality and System Performance</strong>
                                                </a>
                                                <br> <em>Speakers:</em> &nbsp; Mohammad Aliannejadi (University of Amsterdam), Yifei Yuan (University of Copenhagen) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">10:30 - 11:00</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #ffc107;">
                                                    <strong>Coffee Break</strong>
                                                </a>
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">09:30 - 10:30</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;">
                                                    <strong>Sailor: Open Language Models for South-East Asia</strong>
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; Qian Liu (Sea AI lab) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">11:30 - 12:00</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;">
                                                    <strong>From Keyword to Conversational Search: Leveraging Behavioral Data and Deep Learning in E-Commerce</strong>
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; Yupin huang (Amazon) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">12:00 - 12:30</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #000000;">
                                                    <strong>Enhancing LLMs’ Reliability via Generation and Verification</strong>
                                                </a>
                                                <br> <em>Speaker:</em> &nbsp; Wenya Wang (NTU) &nbsp; &nbsp;
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">12:30 - 13:30</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #ffc107;">
                                                    <strong>Lunch</strong>
                                                </a>
                                            </td>
                                        </tr>

                                        <tr class="row100">
                                            <td class="column100 column1" data-column="column1">13:30 - 15:00</td>
                                            <td class="column100 column2" data-column="column2">
                                                <a style="color: #007bff">
                                                    <strong>Poster Session</strong>
                                                </a>
                                            </td>
                                        </tr>
                                    </tbody>

                                </table>
                            </div>
                        </div>
                    </div>

                </div>
            </div>
        </div>
    </section>


    <section id="posters" class="bg-light">
        <div class="container">
            <div class="row">

                <div class="col-lg-8 mx-auto">
                    <h3>Poster Papers</h3>
                    <p style="text-align: justify;">
                        Papers are mainly exported from the EMNLP 2023 and ACL 2023.
                        Oral papers are for 12 minutes plus 3 minutes for immediate questions.
                        Poster boards can accommodate (1m x 1m) sized posters, in either portrait or landscape.
                        We split all the posters into Poster session 1 and Poster session 2, divided as follows, with
                        each containing 13 poster boards.
                    </p>
                    <div id="Oral" data-toggle="collapse" data-parent="#accordion1">
                        <a href="#Oral-list" data-toggle="collapse"><b>Click to see the paper list &darr;</b></a>

                        <div class="accordion1" id="accordion1">

                            <div id="Oral-list" class="collapse" data-parent="#accordion1">
                                <div class="table ver5 m-b-10">
                                    <table data-vertable="ver5">
                                        <tbody>
                                            <tr class="row100">
                                                <td valign="middle" style="font-weight: bold;">Paper session 1
                                                    &nbsp;&nbsp;</td>
                                                <td class="column100" data-column="column1">
                                                    <p style="text-align:left; font-size:small">
                                                        [1] Ye, Hai, & Xie, Qizhe, & Ng, Hwee Tou. <i>Multi-Source
                                                            Test-Time Adaptation as Dueling Bandits for Extractive
                                                            Question Answering</i> (<b>Slot: <a
                                                                style="color: red;">08:15 - 08:30</a></b>)<BR />
                                                        [2] Zhengyuan Liu, Yong Keong Yap, Hai Leong Chieu and Nancy F.
                                                        Chen. <i>Guiding Computational Stance Detection with Expanded
                                                            Stance Triangle Framework</i> (<b>Slot: <a
                                                                style="color: red;">08:30 - 08:45</a></b>)<BR />
                                                        [3] Ahmed Masry*, Parsa Kavehzadeh*, Xuan Long Do, Enamul Hoque,
                                                        Shafiq Joty. <i>UniChart: A Universal Vision-language Pretrained
                                                            Model for Chart Comprehension and Reasoning</i> (<b>Slot: <a
                                                                style="color: red;">08:45 - 09:00</a></b>)<BR />
                                                        [4] Ibrahim Taha Aksu, Devamanyu Hazarika, Shikib Mehri,
                                                        Seokhwan Kim, Dilek Hakkani-Tur, Yang Liu, Mahdi Namazifar.
                                                        <i>CESAR: Automatic Induction of Compositional Instructions for
                                                            Multi-turn Dialogs</i> (<b>Slot: <a
                                                                style="color: red;">09:00 - 09:15</a></b>)<BR />
                                                    </p>
                                                </td>
                                            </tr>


                                            <tr class="row100">
                                                <td valign="middle" style="font-weight: bold;">Paper session 2
                                                    &nbsp;&nbsp;</td>
                                                <td class="column100" data-column="column1">
                                                    <p style="text-align:left; font-size:small">
                                                        [1] Hannan Cao, Liping Yuan, Yuchen Zhang, Hwee Tou Ng.
                                                        <i>Unsupervised Grammatical Error Correction Rivaling Supervised
                                                            Methods</i> (<b>Slot: <a style="color: red;">10:30 -
                                                                10:45</a></b>)<BR />
                                                        [2] Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang,
                                                        Tat-Seng Chua. <i>Robust Prompt Optimization for Large Language
                                                            Models Against Distribution Shifts</i> (<b>Slot: <a
                                                                style="color: red;">10:45 - 11:00</a></b>)<BR />
                                                        [3] Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Ai Ti Aw,
                                                        Nancy F. Chen. <i>Decomposed Prompting for Machine Translation
                                                            between Related Languages using Large Language Models</i>
                                                        (<b>Slot: <a style="color: red;">11:00 - 11:15</a></b>)<BR />
                                                        [4] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao,
                                                        Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua. <i>MolCA: Molecular
                                                            Graph-Language Modeling with Cross-Modal Projector and
                                                            Uni-Modal Adapter.</i> (<b>Slot: <a
                                                                style="color: red;">11:15 - 11:30</a></b>)<BR />
                                                    </p>
                                                </td>
                                            </tr>


                                            <tr class="row100">
                                                <td valign="middle" style="font-weight: bold;">Paper session 3
                                                    &nbsp;&nbsp;</td>
                                                <td class="column100" data-column="column1">
                                                    <p style="text-align:left; font-size:small">
                                                        [1] Jinggui Liang, Lizi Liao. <i>ClusterPrompt: Cluster Semantic
                                                            Enhanced Prompt Learning for New Intent Discovery</i>
                                                        (<b>Slot: <a style="color: red;">14:00 - 14:15</a></b>)<BR />
                                                        [2] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
                                                        Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee.
                                                        <i>LLM-Adapter: An Empirical Study of Adapter-based
                                                            Parameter-Efficient Fine-Tuning for Large Language
                                                            Models</i> (<b>Slot: <a style="color: red;">14:15 -
                                                                14:30</a></b>)<BR />
                                                        [3] Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F
                                                        Chen, Zhengyuan Liu, Diyi Yang. <i>CoAnnotating:
                                                            Uncertainty-Guided Work Allocation between Human and Large
                                                            Language Models for Data Annotation</i> (<b>Slot: <a
                                                                style="color: red;">14:30 - 14:45</a></b>)<BR />
                                                        [4] Quanyu Long, Wenya Wang, Sinno Jialin Pan. <i>Adapt in
                                                            Contexts: Retrieval-Augmented Domain Adaptation via
                                                            In-Context Learning</i> (<b>Slot: <a
                                                                style="color: red;">14:45 - 15:00</a></b>)<BR />
                                                    </p>
                                                </td>
                                            </tr>


                                            <tr class="row100">
                                                <td valign="middle" style="font-weight: bold;">Poster session
                                                    1&nbsp;&nbsp;</td>
                                                <td class="column100" data-column="column1">
                                                    <p style="text-align:left; font-size:small">
                                                        [1] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov,
                                                        Min-Yen Kan. <i>On the Risk of Misinformation Pollution with
                                                            Large Language Models</i> (<b>Board: <a
                                                                style="color: red;">P101</a></b>)<BR />

                                                        [2] Fengzhu Zeng, Wei Gao. <i>Prompt to be Consistent is Better
                                                            than Self-Consistent? Few-Shot and Zero-Shot Fact
                                                            Verification with Pre-trained Language Models</i> (<b>Board:
                                                            <a style="color: red;">P102</a></b>)<BR />

                                                        [3] Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze GAO, Donovan Ong,
                                                        Bin Chen, Jian Su. <i>Battle of the Large Language Models: Dolly
                                                            vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A
                                                            Text-to-SQL Parsing Comparison</i> (<b>Board: <a
                                                                style="color: red;">P105</a></b>)<BR />

                                                        [4] Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang.
                                                        <i>Logic-LM: Empowering large language models with symbolic
                                                            solvers for faithful logical reasoning</i> (<b>Board: <a
                                                                style="color: red;">P106</a></b>)<BR />

                                                        [5] Jiaxi Li and Wei Lu. <i>Contextual Distortion Reveals
                                                            Constituency: Masked Language Models are Implicit
                                                            Parsers</i> (<b>Board: <a
                                                                style="color: red;">P109</a></b>)<BR />

                                                        [6] Shaz Furniturewala, Abhinav Java, Surgan Jandial, Simra
                                                        Shahid, Pragyan Banerjee, Balaji Krishnamurthy, Sumit Bhatia and
                                                        Kokil Jaidka. <i>Evaluating the Efficacy of Prompting Techniques
                                                            for Debiasing Language Model Outputs</i> (<b>Board: <a
                                                                style="color: red;">P110</a></b>)<BR />

                                                        [7] Tan, Qingyu, & Xu, Lu, & Bing, Lidong, & Ng, Hwee Tou.
                                                        <i>Class-Adaptive Self-Training for Relation Extraction with
                                                            Incompletely Annotated Training Data</i> (<b>Board: <a
                                                                style="color: red;">P103</a></b>)<BR />

                                                        [8] Yixi Ding, Yanxia Qin, Qian Liu, Min Yen Kan. <i>CocoSciSum:
                                                            A Scientific Summarization Toolkit with Compositional
                                                            Controllability</i> (<b>Board: <a
                                                                style="color: red;">P104</a></b>)<BR />

                                                        [9] Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana
                                                        Unnithan, Min-Yen Kan. <i>The ACL OCL Corpus: Advancing Open
                                                            Science in Computational Linguistics</i> (<b>Board: <a
                                                                style="color: red;">P107</a></b>)<BR />

                                                        [10] Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, Min-Yen
                                                        Kan. <i>SCITAB: A Challenging Benchmark for Compositional
                                                            Reasoning and Claim Verification on Scientific Tables</i>
                                                        (<b>Board: <a style="color: red;">P108</a></b>)<BR />

                                                        [11] Yeo, Gerard., Jaidka K. <i>The PEACE-Reviews dataset:
                                                            Modeling Cognitive Appraisals in Emotion Text Analysis</i>
                                                        (<b>Board: <a style="color: red;">P111</a></b>)<BR />

                                                        [12] Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos
                                                        Mouratidis, Jing Jiang. <i>ROME: Evaluating Pre-trained
                                                            Vision-Language Models on Reasoning beyond Visual Common
                                                            Sense</i> (<b>Board: <a
                                                                style="color: red;">P112</a></b>)<BR />

                                                        [13] Xiaobing Sun, Jiaxi Li, and Wei Lu. <i>Unraveling Feature
                                                            Extraction Mechanisms in Neural Networks</i> (<b>Board: <a
                                                                style="color: red;">P113</a></b>)<BR />

                                                    </p>
                                                </td>
                                            </tr>


                                            <tr class="row100">
                                                <td valign="middle" style="font-weight: bold;">Poster session
                                                    2&nbsp;&nbsp;</td>
                                                <td class="column100" data-column="column1">
                                                    <p style="text-align:left; font-size:small">
                                                        [1] Xuan Long Do, Bowei Zou, Shafiq Joty, Anh Tai Tran,
                                                        Liangming Pan, Nancy F. Chen, Ai Ti Aw. <i>Modeling What-to-ask
                                                            and How-to-ask for Answer unaware Conversational Question
                                                            Generation</i> (<b>Board: <a
                                                                style="color: red;">P201</a></b>)<BR />

                                                        [2] Rui Cao, Jing Jiang. <i>Modularized Zero-shot VQA with
                                                            Pre-trained Models</i> (<b>Board: <a
                                                                style="color: red;">P202</a></b>)<BR />

                                                        [3] Bin Wang, Zhengyuan Liu, Nancy F. Chen. <i>Instructive
                                                            Dialogue Summarization with Query Aggregations</i>
                                                        (<b>Board: <a style="color: red;">P205</a></b>)<BR />

                                                        [4] Huy Quang Dao, Lizi Liao, Dung D. Le, Yuxiang Nie.
                                                        <i>Reinforced Target-driven Conversational Promotion</i>
                                                        (<b>Board: <a style="color: red;">P206</a></b>)<BR />

                                                        [5] Ibrahim Taha Aksu, Min-Yen Kan and Nancy F. Chen.
                                                        <i>Zero-shot Adaptive Prefixes for Dialogue State Tracking
                                                            Domain Adaptation</i> (<b>Board: <a
                                                                style="color: red;">P209</a></b>)<BR />

                                                        [6] Guangsheng Bao, Zhiyang Teng, Hao Zhou, Jianhao Yan, Yue
                                                        Zhang. <i>Non-Autoregressive Document-Level Machine
                                                            Translation</i> (<b>Board: <a
                                                                style="color: red;">P210</a></b>)<BR />

                                                        [7] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William
                                                        Yang Wang, Min-Yen Kan, Preslav Nakov. <i>Fact-Checking Complex
                                                            Claims with Program-Guided Reasoning</i> (<b>Board: <a
                                                                style="color: red;">P203</a></b>)<BR />

                                                        [8] Muhammad Reza Qorib, Hwee Tou Ng. <i>System Combination via
                                                            Quality Estimation for Grammatical Error Correction</i>
                                                        (<b>Board: <a style="color: red;">P204</a></b>)<BR />

                                                        [9] Tan, Qingyu, & Ng, Hwee Tou, & Bing, Lidong. <i>Towards
                                                            Benchmarking and Improving the Temporal Reasoning Capability
                                                            of Large Language Models</i> (<b>Board: <a
                                                                style="color: red;">P207</a></b>)<BR />

                                                        [10] Ruichao Yang, Wei Gao, Jing Ma, Zhiwei Yang. <i>WSDMS:
                                                            Debunk Fake News via Weakly Supervised Detection of
                                                            Misinforming Sentences with Contextualized Social Wisdom</i>
                                                        (<b>Board: <a style="color: red;">P208</a></b>)<BR />

                                                        [11] Ratish Puduppully, Parag Jain, Nancy F. Chen and Mark
                                                        Steedman. <i>Multi-Document Summarization with Centroid-Based
                                                            Pretraining</i> (<b>Board: <a
                                                                style="color: red;">P211</a></b>)<BR />

                                                        [12] Mathieu Ravaut, Shafiq Joty, Nancy F. Chen. <i>Unsupervised
                                                            Summarization Re-ranking</i> (<b>Board: <a
                                                                style="color: red;">P212</a></b>)<BR />

                                                        [13] Zhiqiang Hu, Nancy F. Chen, Roy Ka-Wei Lee. <i>Adapter-TST:
                                                            A Parameter Efficient Method for Multiple-Attribute Text
                                                            Style Transfer</i> (<b>Board: <a
                                                                style="color: red;">P213</a></b>)<BR />

                                                    </p>
                                                </td>
                                            </tr>


                                        </tbody>
                                    </table>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
    </section>


    <section id="speakers" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h3>Speakers</h3>
                    <p>
                        The following speakers are invited to give keynotes at IDSNLP 2024. Please click the profile image to view the detailed content of the talk.
                    </p>
                    <div class="accordion" id="accordion">
                        <div class="table-responsive">
                            <table class="table">
                                <tbody>
                                    <tr align="center">
                                        <td>
                                            <a href="#Preslav" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/preslav-nakov.jpeg" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>Preslav Nakov</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>
                                        <td>
                                            <a href="#Farah" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/Farah-Benamara.jpeg" class="hover2"
                                                        align="center" height="185">
                                                    <figcaption>
                                                        <h5>Farah Benamara</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>
                                        <td>
                                            <a href="#Vivian" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/Vivian-Chen.jpg" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>Vivian Chen</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>

                                        <td>
                                            <a href="#Tanya" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/tanya-goyal.jpeg" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>Tanya Goyal</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>

                                    </tr>
                                </tbody>
                            </table>
                        </div>


                        <div id="Preslav" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong>Jais and Jais-chat: Building the World's Best Open Arabic-Centric
                            Foundation and Instruction-Tuned Open Generative Large Language Models
                            <br>
                            <strong>Speaker: </strong><a
                                href="https://mbzuai.ac.ae/study/faculty/preslav-nakov/">Preslav Nakov</a>
                            <br>
                            <br>
                            <p style="text-align: justify;">
                                <strong> Abstract: </strong> I will discuss Jais and Jais-chat, two state-of-the-art
                                Arabic-centric foundation and instruction-tuned open generative large language models
                                (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained
                                on a mixture of Arabic and English texts, including source code in various programming
                                languages. The models demonstrate better knowledge and reasoning capabilities in Arabic
                                than previous open Arabic and multilingual models by a sizable margin, based on
                                extensive evaluation. Moreover, they are competitive in English compared to
                                English-centric open models of similar size, despite being trained on much less English
                                data. I will discuss the training, the tuning, the safety alignment, and the evaluation,
                                as well as the lessons we learned.
                            </p>
                            <br>
                            <p style="text-align: justify;">
                                <strong>Bio:</strong>
                                Dr. Preslav Nakov is Professor and Department Chair for NLP at the Mohamed bin Zayed
                                University of Artificial Intelligence. Previously, he was Principal Scientist at the
                                Qatar Computing Research Institute, HBKU, where he led the Tanbih mega-project,
                                developed in collaboration with MIT, which aims to limit the impact of "fake news",
                                propaganda and media bias by making users aware of what they are reading, thus promoting
                                media literacy and critical thinking. He received his PhD degree in Computer Science
                                from the University of California at Berkeley, supported by a Fulbright grant. He is
                                Chair-Elect of the European Chapter of the Association for Computational Linguistics
                                (EACL), Secretary of ACL SIGSLAV, and Secretary of the Truth and Trust Online board of
                                trustees. Formerly, he was PC chair of ACL 2022, and President of ACL SIGLEX. He is also
                                member of the editorial board of several journals including Computational Linguistics,
                                TACL, ACM TOIS, IEEE TASL, IEEE TAC, CS&L, NLE, AI Communications, and Frontiers in AI.
                                He authored a Morgan & Claypool book on Semantic Relations between Nominals, two books
                                on computer algorithms, and 250+ research papers. He received a Best Paper Award at ACM
                                WebSci'2022, a Best Long Paper Award at CIKM'2020, a Best Demo Paper Award (Honorable
                                Mention) at ACL'2020, a Best Task Paper Award (Honorable Mention) at SemEval'2020, a
                                Best Poster Award at SocInfo'2019, and the Young Researcher Award at RANLP’2011. He was
                                also the first to receive the Bulgarian President's John Atanasoff award, named after
                                the inventor of the first automatic electronic digital computer. His research was
                                featured by over 100 news outlets, including Reuters, Forbes, Financial Times, CNN,
                                Boston Globe, Aljazeera, DefenseOne, Business Insider, MIT Technology Review, Science
                                Daily, Popular Science, Fast Company, The Register, WIRED, and Engadget, among others.
                            </p><br>
                            <br>
                        </div>

                        <div id="Farah" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong>Generative IA for Social Good: A Myth or a Reality?
                            <br>
                            <strong>Speaker: </strong><a href="https://www.irit.fr/~Farah.Benamara/">Farah Benamara</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract: </strong>
                                Linguistically-informed processing of unstructured textual interactions offers an
                                important testing ground for hybrid AI and AI for social good, in particular when the
                                attempt is to automatically under- stand beyond what is said. This talk is about the
                                implicit nature of linguistic expressions investigating the role of context in their
                                automatic processing: If humans need context, what about machines? I attempt to answer
                                this question on two particular NLP applications: Hate speech detection and crisis
                                management. I review main findings of current studies and question the use of generative
                                IA models in applications with great social and ethical implications for society.
                            </p>
                            <br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Farah Benamara is a Full Professor of computer science at Toulouse University Paul
                                Sabatier. She is member of IRIT laboratory and co-head of the MELODI group. Her research
                                concerns Natural Language Processing and focuses on the development of semantic and
                                pragmatic models for language understanding with a particular attention on evaluative
                                language processing, discourse processing and information extraction from texts. She
                                published more than 100 publications in peer-reviewed international conference and
                                journal papers. She has been designed to be area chair at ACL 2019, EACL 2021, EACL 2024
                                and Senior Area Chair at NAACL 2024. She is member of the editorial board of the journal
                                of Dialogue and Discourse, IEEE Affective Computing and Traitement Automatique des
                                Langues. She co-edited a special issue on contextual phenomena in evaluative language
                                processing in the journal of Computational Linguistics. She is PI of several ongoing
                                projects among which DesCartes at CNRA@CREATE Singapore on hybrid IA for NLP,
                                Sterheotypes an EU project on the detection of racial stereotypes, QualityOnto an
                                ANR-DFG project on fact-checking for knowledge graph validation and finally INTACT, a
                                CNRS prematuration project on NLP-based crisis management from social media.
                            </p><br>
                            <br>
                        </div>

                        <div id="Vivian" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong> From Bots to Buddies: Making Conversational Agents More Human-Like
                            <br>
                            <strong>Speaker: </strong><a href="https://www.csie.ntu.edu.tw/~yvchen/">Vivian Chen</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract: </strong>
                                While today's conversational agents are equipped with impressive capabilities, there
                                remains a clear distinction between the intuitive prowess of humans and the operational
                                limits of machines. An example of this disparity is evident in the human ability to
                                infer implicit intents from users' utterances, subsequently guiding conversations toward
                                specific topics or recommending appropriate tasks or products. This talk aims to elevate
                                conversational agents to a more human-like realm, enhancing user experience and
                                practicality. By exploring innovative strategies and frameworks that leverages
                                commonsense knowledge, we delve into the potential ways conversational agents can evolve
                                to offer more seamless, contextually aware, and user-centric interactions. The goal is
                                to not only close the gap between human and machine interactions but also to unlock new
                                possibilities in how conversational agents can be utilized in our daily lives.
                            </p><br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Yun-Nung (Vivian) Chen is currently an associate professor in the Department of
                                Computer Science & Information Engineering at National Taiwan University. She earned her
                                Ph.D. degree from Carnegie Mellon University, where her research interests focus on
                                spoken dialogue systems and natural language processing. She was recognized as the
                                Taiwan Outstanding Young Women in Science and received Google Faculty Research Awards,
                                Amazon AWS Machine Learning Research Awards, MOST Young Scholar Fellowship, and FAOS
                                Young Scholar Innovation Award. Her team was selected to participate in the first Alexa
                                Prize TaskBot Challenge in 2021. Prior to joining National Taiwan University, she worked
                                in the Deep Learning Technology Center at Microsoft Research Redmond.
                            </p><br>
                            <br>
                        </div>

                        <div id="Tanya" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong>Evaluation in the era of GPT-4
                            <br>
                            <strong>Speaker: </strong><a href="https://tagoyal.github.io/">Tanya Goyal</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract:</strong>
                                As large language models become more embedded in user applications, there is a push to
                                align their outputs with human preferences. But human preferences are highly subjective,
                                making both model alignment and evaluation extremely challenging. In this talk, I will
                                first outline work that highlights this subjectivity, for a relatively well-defined
                                tasks like summarization, and its effects on downstream model evaluations. Next, I will
                                discuss how effectively trained models can capture human preferences and the impact of
                                integrating these models into RLHF pipelines.
                            </p>
                            <br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Tanya Goyal is an incoming (Fall 2024) assistant professor of Computer Science at
                                Cornell University. For the 2023-2024 academic year, she is a postdoctoral researcher at
                                the Princeton Language and Intelligence (PLI) group. Her current research focuses on
                                designing scalable and cost-effective evaluation techniques for LLMs. Particularly, she
                                is interested in understanding and modeling the subjectivity in human feedback, and how
                                this affects both evaluation and training of LLMs at scale. Previously, she received her
                                Ph.D. in computer science from the University of Texas at Austin in 2023, advised by Dr.
                                Greg Durrett. Her thesis research focused on building tools to automatically detect
                                attribution errors in generated text.
                            </p>
                            <br>
                            <br>
                        </div>

                        <div class="table-responsive">
                            <table class="table">
                                <tbody>
                                    <tr align="center">
                                        <td>
                                            <a href="#Diyi" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/Diyi_Yang.jpg" class="hover2" height="185">
                                                    <figcaption>
                                                        <h5>Diyi Yang</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>
                                        <td>
                                            <a href="#Joao" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/joao-sedoc.webp" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>João Sedoc</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>

                                        <td>
                                            <a href="#Daniel" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/Daniel Preotiuc.jpeg" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>Daniel Preoțiuc-Pietro</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>
                                        <td>
                                            <a href="#Huda" data-toggle="collapse">
                                                <figure>
                                                    <img src="images/speaker/HudaKhayrallah.jpg" class="hover2"
                                                        height="185">
                                                    <figcaption>
                                                        <h5>Huda Khayrallah</h5>
                                                    </figcaption>
                                                </figure>
                                            </a>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div id="Diyi" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong> Human-AI Interaction in the Age of LLMs
                            <br>
                            <strong>Speaker: </strong><a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract:</strong>
                                Large language models have revolutionized the way humans interact with AI systems,
                                transforming a wide range of fields and disciplines. In this talk, I share two distinct
                                approaches to empowering human-AI interaction using LLMs. The first one explores how
                                large language models transform computational social science, and how human-AI
                                collaboration can reduce costs and improve the efficiency of social science research.
                                The second part focuses on social skill learning via LLMs by empowering therapists with
                                LLM-empowered feedback and deliberative practices. These two works demonstrate how
                                human-AI interaction via LLMs can foster positive change.
                            </p>
                            <br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Diyi Yang is an assistant professor in the Computer Science Department at Stanford
                                University. Her research focuses on natural language processing for social impact. She
                                has received multiple best paper awards and recognitions at leading conferences in NLP
                                and HCI. She is a recipient of IEEE “AI 10 to Watch” (2020), Intel Rising Star Faculty
                                Award (2021), Samsung AI Researcher of the Year (2021), Microsoft Research Faculty
                                Fellowship (2021), NSF CAREER Award (2022), and an ONR Young Investigator Award (2023).
                            </p><br>
                            <br>
                        </div>

                        <div id="Joao" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong>New Dimensions for the Evaluation of Conversational Agents
                            <br>
                            <strong>Speaker: </strong><a href="https://www.stern.nyu.edu/faculty/bio/joao-sedoc">João
                                Sedoc</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract: </strong>
                                The rapid advances in large language models brought about disruptive innovations in the
                                field of conversational agents. However, recent advances also present new challenges in
                                evaluating the quality of such systems, as well as the underlying models and methods. As
                                conversational agents increasingly match and or even surpass human performance in
                                dimensions like 'coherence,' we must shift our focus to the qualities of conversational
                                agents that are fundamental to human-like conversation (e.g., empathy and emotion). In
                                this talk, I will focus on how we can integrate psychological metrics for evaluating
                                conversational agents along dimensions such as emotion, empathy, and user traits.
                            </p><br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. João Sedoc is an Assistant Professor of Information Systems in the Department of
                                Technology, Operations and Statistics at New York University Stern School of Business.
                                He is also affiliated with the Center for Datascience ML^2 Lab at NYU. His research
                                areas are at the intersection of machine learning and natural language processing. His
                                interests include conversational agents, hierarchical models, deep learning, and time
                                series analysis. Before joining NYU Stern, he worked as an Assistant Research Professor
                                in the Department of Computer Science at Johns Hopkins University. He received his PhD
                                in Computer and Information Science from the University of Pennsylvania.
                            </p><br>
                            <br>
                        </div>

                        <div id="Daniel" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong> Modular Language Modeling through Model Merging
                            <br>
                            <strong>Speaker: </strong><a href="https://www.preotiuc.ro/">Daniel Preoțiuc-Pietro</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract:</strong>
                                Pre-trained language models are the cornerstone of most NLP applications and their
                                performance is dependent on using large and diverse data sets. Combining knowledge of
                                multiple data sets in a single model either in pretraining or fine-tuning can lead to
                                better overall performance on in-domain data and can better generalize on out-of-domain
                                data. This talk will present methods and experiments with model merging, defined as
                                combining multiple models into a single one in parameter space without access to data or
                                retraining. This enables models to be modular by design, where models trained on
                                individual data sets can be dynamically used, combined or, if needed, removed under
                                arbitrary constraints. Merging is compute and parameter efficient and allows leveraging
                                models without access to potentially private data used in their training.
                            </p><br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Daniel Preoțiuc-Pietro is a Senior Research Scientist and the manager of the NLP
                                Platforms group in the Bloomberg AI Engineering group. The group's work powers products
                                for news, financial documents, social media and search. His main research interests are
                                on understanding and modelling the social, pragmatic and temporal aspects of text,
                                especially from social media, with applications in domains such as Psychology, Law,
                                Political Science and Journalism. His research was featured in popular press including
                                the Washington Post, BBC, Scientific American or FiveThirtyEight. He is a co-organizer
                                of the Natural Legal Language processing workshop series. Prior to joining Bloomberg, he
                                obtained his PhD from the University of Sheffield and was a postdoctoral researcher at
                                the University of Pennsylvania.
                            </p><br>
                            <br>
                        </div>

                        <div id="Huda" class="collapse" data-parent="#accordion">
                            <strong>Title: </strong> Perplexity-Driven Case Encoding Needs Augmentation for
                            CAPITALIZATION Robustness
                            <br>
                            <strong>Speaker: </strong><a href="https://khayrallah.github.io/">Huda Khayrallah</a>
                            <br>
                            <br>
                            <p style="text-align: justify;"> <strong> Abstract:</strong>
                                For most NLP models, upper and lower case letters are represented with distinct
                                code-points. In contrast, most people naturally connect upper and lower-cased letters as
                                highly similar and therefore expect NLP models to perform similarly on inputs that only
                                differ in casing. However, that is often not the case, and NLP models are often unstable
                                on non-standard casings. Subword segmentation methods (e.g., BPE (Sennrich et al., 2016)
                                and SPM (Kudo and Richardson, 2018)) handle the sparsity introduced by a variety of
                                linguistic features (e.g. concatenative morphology) by learning a segmentation of words
                                into shorter sequences of characters. However, such methods do not currently handle the
                                sparsity introduced by casing well and can lead to terrible quality on ALL CAPS data.
                                Prior work (Berard et al., 2019; Etchegoyhen and Gete, 2020) overcame the quality drop
                                in machine translation but did so in a way that breaks the encoding optimality of
                                perplexity driven methods, leading to impractical sequence length/runtime. In this work,
                                we re-encode capitalization to allow the perplexity-driven subword segmentation model to
                                learn how to best segment this linguistic feature. Naturally occurring data accurately
                                describes the prevalence of capitalization but underestimates the importance humans
                                ascribe to capitalization robustness. We propose data augmentation to fill this gap.
                                Overall, we increase translation quality on data with different casings (compared to
                                standard SPM), with minimal impact on decoding speed on standard cased data and large
                                speed improvements on ALL CAPS data.
                            </p><br>
                            <p style="text-align: justify;"> <strong>Bio:</strong>
                                Dr. Huda Khayrallah is a senior researcher at Microsoft, working on the Microsoft
                                Translator team. She holds a PhD in computer science from The Johns Hopkins University
                                (JHU), where she was advised by Philipp Koehn. She also holds a bachelor’s in computer
                                science from UC Berkeley. She has worked on a variety of topics in machine translation
                                and NLP including: low resource MT, noisy data in MT, domain adaptation, chatbots, and
                                more.
                            </p><br>
                            <br>
                        </div>

                    </div>
                </div>
            </div>
    </section>


    <section id="organizers" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h2>Organizers</h2>

                    <!-- <span>
                        <a href="http://nus.edu.sg/">
                            <img src="images/nus.png" width="190" style="margin-right: 30px">
                        </a>
                        <a href="https://www.ntu.edu.sg">
                            <img src="images/ntu.png" width="230" style="margin-right: 20px">
                        </a>
                        <a href="https://www.smu.edu.sg/">
                            <img src="images/smu.png" width="230" style="margin-right: 20px">
                        </a>
                        <a href="https://www.sutd.edu.sg/">
                            <img src="images/sutd.png" width="180" style="margin-left: 100px">
                        </a>
                        <a href="https://www.a-star.edu.sg/i2r">
                            <img src="images/i2r.png" width="190" style="margin-left: 120px">
                        </a>
                    </span> -->

                    <p>
                    <table class="table">
                        <tbody>
                            <tr>
                                <td>General Chair:</td>
                                <td>
                                    <p><a href="https://kokiljaidka.wordpress.com"> Kokil Jaidka</a>, National
                                        University of Singapore</p>
                                </td>
                            </tr>

                            <tr>
                                <td>PC Chair:</td>
                                <td>
                                    <p><a href="https://sites.google.com/view/gaowei"> Wei Gao</a>, Singapore Management
                                        University</p>
                                </td>
                            </tr>

                            <tr>
                                <td>Organizing Committee:</td>
                                <td>
                                    <p><a href="https://wing.comp.nus.edu.sg/people/"> Yanxia Qin</a>, National
                                        University of Singapore</p>
                                    <p><a href="http://haofei.vip/"> Hao Fei</a>, National University of Singapore</p>
                                    <p><a href="https://dengyang17.github.io/"> Yang Deng</a>, National University of
                                        Singapore</p>
                                    <p><a href="https://wing.comp.nus.edu.sg/people/"> Sun Shuo</a>, Institute for
                                        Infocomm Research</p>
                                    <p><a href="https://suzyahyah.github.io/about/"> Suzanna Sia</a>, Johns Hopkins
                                        University</p>
                                    <p><a href="#"> Gerard Yeo</a>, National University of Singapore</p>
                                </td>
                            </tr>

                            <tr>
                                <td>Co-organizers:</td>
                                <td>
                                    <p><a href="https://www.comp.nus.edu.sg/~kanmy">Min-Yen Kan</a>, National University
                                        of Singapore</p>
                                    <p><a href="https://www.comp.nus.edu.sg/~nght">Hwee Tou Ng</a>, National University
                                        of Singapore</p>
                                    <p><a href="https://www.chuatatseng.com/">Tat-Seng Chua</a>, National University of
                                        Singapore</p>
                                    <p><a href="https://raihanjoty.github.io">Shafiq Joty</a>, Nanyang Technological
                                        University</p>
                                    <p><a href="https://sites.google.com/site/nancyfchen/home">Nancy Chen</a>, Institute
                                        for Infocomm Research</p>
                                    <p><a href="http://www.colips.org/~sujian/">Jian Su</a>, Institute for Infocomm
                                        Research</p>
                                    <p><a href="http://www.mysmu.edu/faculty/jingjiang">Jing Jiang</a>, Singapore
                                        Management University</p>
                                    <p><a href="https://istd.sutd.edu.sg/people/faculty/lu-wei">Wei Lu</a>, Singapore
                                        University of Technology and Design</p>
                                    <p><a href="https://people.sutd.edu.sg/~sporia/">Soujanya Poria</a>, Singapore
                                        University of Technology and Design</p>
                                </td>
                            </tr>


                        </tbody>
                    </table>

                    </p>

                </div>
            </div>
        </div>
    </section>


    <section id="location" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h2>Location</h2>
                    <p> <b>IDSNLP 2024</b> will be held at <b>Innovation 4.0, 3 Research Link, Singapore 117602</b>.
                    </p>
                    <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3988.7998239328067!2d103.7731851752785!3d1.2946430986930662!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x31da1b112ef2d375%3A0x30d60043abb3e0f2!2sInstitute%20of%20Data%20Science!5e0!3m2!1szh-CN!2shk!4v1714816350018!5m2!1szh-CN!2shk" 
                        width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade">
                    </iframe>
                </div>
            </div>
        </div>
    </section>


    <section id="contact" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <h2>Contact Us</h2>
                    <p id="h.p_4XXDjs_mNTFP" class="zfr3Q">Please feel free to reach out if you have any inquiries:
                        <a href="mailto:zhiyuan_hu@u.nus.edu" target="_blank" rel="noopener">Zhiyuan Hu</a> and <a
                            href="mailto:mingzhe@nus.edu.sg" target="_blank" rel="noopener">Mingzhe Du</a>.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <footer class="py-5 bg-dark">
        <div class="container" style="text-align: center;">
            <p> <span style="color: white; ">Copyright &copy; IDSNLP 2024 | <a href="https://ids.nus.edu.sg/">NUS
                        IDS</a> </p>
        </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>


    <script src="vendor/bootstrap/js/popper.js"></script>
    <script src="vendor/select2/select2.min.js"></script>
    <script src="js/main.js"></script>

</body>

</html>